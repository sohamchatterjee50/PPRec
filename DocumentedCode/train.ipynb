{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPRec Training Modules, BPE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Modules \"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "\"\"\"  Knowledge Aware News Encoder which uses self attention and cross attention modules    \"\"\"\n",
    "class KnowledgeAwareNewsEncoder(nn.Module):\n",
    "    def __init__(self,hparams,\n",
    "        word2vec_embedding=None,\n",
    "        seed=None,\n",
    "        **kwargs,):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.word_self_attention = torch.nn.MultiheadAttention(hparams.embed_dim,hparams.head_num,batch_first=True)\n",
    "        self.entity_self_attention = torch.nn.MultiheadAttention(hparams.embed_dim,hparams.head_num,batch_first=True)\n",
    "        self.word_cross_attention = torch.nn.MultiheadAttention(hparams.embed_dim,hparams.head_num, batch_first=True)\n",
    "        self.entity_cross_attention = torch.nn.MultiheadAttention(hparams.embed_dim,hparams.head_num, batch_first=True)\n",
    "\n",
    "        \n",
    "        self.word2vec = nn.Embedding.from_pretrained(word2vec_embedding)\n",
    "        self.entity2vec = nn.Embedding.from_pretrained(word2vec_embedding)\n",
    "        self.final_attention_layer = torch.nn.MultiheadAttention(hparams.embed_dim,hparams.head_num, batch_first=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, words, entities):\n",
    "        word_embeddings = self.word2vec(words)\n",
    "        entity_embeddings =  self.entity2vec(entities)\n",
    "        word_embeddings = torch.reshape(word_embeddings,(word_embeddings.shape[0],word_embeddings.shape[1]*word_embeddings.shape[2],word_embeddings.shape[3]))\n",
    "        entity_embeddings = torch.reshape(entity_embeddings,(entity_embeddings.shape[0],entity_embeddings.shape[1]*entity_embeddings.shape[2],entity_embeddings.shape[3]))\n",
    "        \n",
    "        \"\"\" Word level self attention \"\"\"\n",
    "        word_self_attn_output,_ = self.word_self_attention(word_embeddings, word_embeddings, word_embeddings)\n",
    "        \n",
    "        \"\"\" Entity(in this case NER clusters) level self attention \"\"\"\n",
    "        entity_self_attn_output,_ = self.entity_self_attention(entity_embeddings, entity_embeddings, entity_embeddings)\n",
    "        \n",
    "        \"\"\" Cross attention between words and entities   \"\"\"\n",
    "        word_cross_output,_ = self.word_cross_attention(word_embeddings,entity_embeddings,entity_embeddings)\n",
    "        entity_cross_output,_ = self.word_cross_attention(entity_embeddings, word_embeddings, word_embeddings)\n",
    "        \n",
    "\n",
    "        word_output = torch.add(word_self_attn_output,word_cross_output)\n",
    "        entity_output = torch.add(entity_self_attn_output,entity_cross_output)\n",
    "        news_encoder,_ = self.final_attention_layer(word_output, entity_output, entity_output)\n",
    "        return news_encoder\n",
    "\n",
    "\n",
    "\n",
    "class TimeAwarePopularityEncoder(nn.Module):\n",
    "    def __init__(self,word2vec_embedding=None,\n",
    "        seed=None,\n",
    "        **kwargs,):\n",
    "        super(TimeAwarePopularityEncoder, self).__init__()\n",
    "        self.word2vec = nn.Embedding.from_pretrained(word2vec_embedding)\n",
    "        self.news_model = nn.Sequential(\n",
    "          nn.Linear(768,256),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(256,256),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(256,128),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(128,1,bias=False)\n",
    "        )\n",
    "        self.dense = nn.Linear(30,1)\n",
    "        \n",
    "        \n",
    "        self.recency_model = nn.Sequential(\n",
    "            nn.Linear(768,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,1,bias=False)\n",
    "        )\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(31,128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.ctr_model = nn.Sigmoid()\n",
    "        self.combined_embed = nn.Linear(1,1)\n",
    "\n",
    "\n",
    "    def forward(self,news, recency, ctr):\n",
    "        news_embed = self.word2vec(news)\n",
    "        recency_embed = self.word2vec(recency)\n",
    "        ctr_embed = self.word2vec(ctr)\n",
    "        content_score = self.news_model(news_embed)\n",
    "        recency_score = self.recency_model(recency_embed)\n",
    "        recency_tensor = recency.unsqueeze(-1)\n",
    "\n",
    "        combined_input = torch.cat([news,recency_tensor],2)\n",
    "        combined_input = combined_input.to(torch.float32)\n",
    "\n",
    "        combined_score = self.gate(combined_input)\n",
    "        final_content_score = content_score.squeeze(-1)\n",
    "        final_content_score = self.dense(final_content_score)\n",
    "        \n",
    "        combined_prefinal_score = (1-combined_score)*recency_score+combined_score*final_content_score\n",
    "        ctr_score = self.ctr_model(ctr_embed)\n",
    "\n",
    "        combined_final_score = self.combined_embed(combined_prefinal_score)\n",
    "        return ctr_score+combined_final_score\n",
    "    \n",
    "class ContentPopularityJointAttention(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Implementation of the content-popularity joint attention module\n",
    "    for the popularity-aware user encoder.\n",
    "\n",
    "    This is based on formula (2) in 3.4 of the paper.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_clicked: int, m_size: int, p_size: int, weight_size: int):\n",
    "        super().__init__()\n",
    "        self.Wu = nn.Parameter(torch.rand(weight_size, m_size + p_size))\n",
    "        self.b = nn.Parameter(torch.rand(weight_size))\n",
    "\n",
    "        self.weight_size = weight_size\n",
    "        self.m_size = m_size\n",
    "        self.p_size = p_size\n",
    "        self.max_clicked = max_clicked\n",
    "\n",
    "    def forward(self, m: torch.Tensor, p: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Calculates the user interest embeddings u, based on the\n",
    "        the popularity embeddings p, and the contextual news\n",
    "        representations m.\n",
    "\n",
    "        m is a tensor of shape (batch_size, max_clicked, m_size)\n",
    "        p is a tensor of shape (batch_size, max_clicked, p_size)\n",
    "        u is a tensor of shape (batch_size, m_size)\n",
    "        where max_clicked is the number of clicked articles by the user.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(m.size()) == 3\n",
    "        batch_size, max_clicked, m_size = m.size()\n",
    "        assert m_size == self.m_size\n",
    "        assert max_clicked == self.max_clicked\n",
    "\n",
    "        assert len(p.size()) == 3\n",
    "        assert p.size(0) == batch_size\n",
    "        assert p.size(1) == max_clicked\n",
    "        assert p.size(2) == self.p_size\n",
    "\n",
    "        mp = torch.cat((m, p), dim=2)  # (batch_size, max_clicked, m_size + p_size)\n",
    "        assert len(mp.size()) == 3\n",
    "        assert mp.size(0) == batch_size\n",
    "        assert mp.size(1) == max_clicked\n",
    "        assert mp.size(2) == self.m_size + self.p_size\n",
    "\n",
    "        Wu_mp = torch.matmul(mp, self.Wu.T)  # (batch_size, max_clicked, weight_size)\n",
    "        assert len(Wu_mp.size()) == 3\n",
    "        assert Wu_mp.size(0) == batch_size\n",
    "        assert Wu_mp.size(1) == max_clicked\n",
    "        assert Wu_mp.size(2) == self.weight_size\n",
    "\n",
    "        tanh_Wu_mp = torch.tanh(Wu_mp)  # (batch_size, max_clicked, weight_size)\n",
    "        assert len(tanh_Wu_mp.size()) == 3\n",
    "        assert tanh_Wu_mp.size(0) == batch_size\n",
    "        assert tanh_Wu_mp.size(1) == max_clicked\n",
    "        assert tanh_Wu_mp.size(2) == self.weight_size\n",
    "\n",
    "        b_tanh_Wu_mp = torch.matmul(tanh_Wu_mp, self.b)  # (batch_size, max_clicked)\n",
    "        assert len(b_tanh_Wu_mp.size()) == 2\n",
    "        assert b_tanh_Wu_mp.size(0) == batch_size\n",
    "        assert b_tanh_Wu_mp.size(1) == max_clicked\n",
    "\n",
    "        sum_b_tanh_Wu_mp = torch.sum(b_tanh_Wu_mp, dim=1)  # (batch_size)\n",
    "        assert len(sum_b_tanh_Wu_mp.size()) == 1\n",
    "        assert sum_b_tanh_Wu_mp.size(0) == batch_size\n",
    "\n",
    "        a = torch.div(\n",
    "            b_tanh_Wu_mp, sum_b_tanh_Wu_mp.unsqueeze(1)\n",
    "        )  # (batch_size, max_clicked)\n",
    "        assert len(a.size()) == 2\n",
    "        assert a.size(0) == batch_size\n",
    "        assert a.size(1) == max_clicked\n",
    "\n",
    "        am = torch.mul(a.unsqueeze(2), m)  # (batch_size, max_clicked, m_size)\n",
    "        assert len(am.size()) == 3\n",
    "        assert am.size(0) == batch_size\n",
    "        assert am.size(1) == max_clicked\n",
    "        assert am.size(2) == self.m_size\n",
    "\n",
    "        u = torch.sum(am, dim=1)  # (batch_size, m_size)\n",
    "        assert len(u.size()) == 2\n",
    "        assert u.size(0) == batch_size\n",
    "        assert u.size(1) == self.m_size\n",
    "\n",
    "        return u\n",
    "    \n",
    "\n",
    "class PopularityAwareUserEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hparams,\n",
    "        word2vec_embedding=None,\n",
    "                 seed=None,\n",
    "                **kwargs,):\n",
    "                 super().__init__()\n",
    "\n",
    "                 self.word2vec = nn.Embedding.from_pretrained(word2vec_embedding)\n",
    "        \n",
    "                 self.pop_embed = nn.Embedding.from_pretrained(word2vec_embedding)\n",
    "                 self.news_self_attention = torch.nn.MultiheadAttention(hparams.embed_dim,hparams.head_num, batch_first=True)\n",
    "                 self.cpja = ContentPopularityJointAttention(hparams.max_clicked, hparams.m_size, hparams.p_size,hparams.weight_size)\n",
    "                 self.max_clicked = hparams.max_clicked\n",
    "                 self.title_length = hparams.title_size\n",
    "\n",
    "    def forward(self,news,popularity):\n",
    "        \n",
    "        \n",
    "        popularity_embedding = self.pop_embed(popularity)\n",
    "        popularity_embedding = popularity_embedding.squeeze(axis=2)\n",
    "        news_embedding = self.word2vec(news)\n",
    "        news_embedding = torch.reshape(news_embedding,(news_embedding.shape[0],news_embedding.shape[1]*news_embedding.shape[2],news_embedding.shape[3]))\n",
    "        news_attention_embedding,_ = self.news_self_attention(news_embedding,news_embedding,news_embedding)\n",
    "\n",
    "        news_attention_embedding = torch.reshape(news_attention_embedding,(news_attention_embedding.shape[0],self.max_clicked,self.title_length,news_attention_embedding.shape[2]))\n",
    "        news_attention_embedding = torch.mean(news_attention_embedding, dim=2, keepdim=False)\n",
    "        \n",
    "        pop_aware_user_encoder = self.cpja(news_attention_embedding,popularity_embedding)\n",
    "        return pop_aware_user_encoder\n",
    "    \n",
    "\n",
    "\n",
    "class PPRec(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    Implementation of PPRec. Figure 2 in the paper shows the architecture.\n",
    "    Outputs a ranking score for some candidate news articles.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams_pprec,\n",
    "        word2vec_embedding= None \n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.knowledge_news_model  = KnowledgeAwareNewsEncoder(hparams_pprec,torch.from_numpy(word2vec_embedding),seed=123)\n",
    "        self.user_model = PopularityAwareUserEncoder(hparams_pprec, word2vec_embedding=torch.from_numpy(word2vec_embedding), seed=123)\n",
    "        self.time_news_model = TimeAwarePopularityEncoder(word2vec_embedding=torch.from_numpy(word2vec_embedding), seed=123)\n",
    "\n",
    "        \n",
    "        self.aggregator_gate = nn.Sequential(\n",
    "            nn.Linear(5,5),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.title_size = hparams_pprec.title_size\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        title, entities, ctr, recency, hist_title, hist_popularity\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Returns the ranking scores for a batch of candidate news articles, given the user's\n",
    "        past click history.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        knowledge_news_embed = self.knowledge_news_model(title, entities)\n",
    "        \n",
    "        time_aware_pop = self.time_news_model(title, recency, ctr)\n",
    "        \n",
    "        user_embed = self.user_model(hist_title,hist_popularity)\n",
    "        \n",
    "        time_aware_pop = torch.mean(time_aware_pop, dim=2, keepdim=False)\n",
    "        knowledge_news_embed = torch.reshape(knowledge_news_embed, (knowledge_news_embed.shape[0],int(knowledge_news_embed.shape[1]/self.title_size), self.title_size,knowledge_news_embed.shape[2]))\n",
    "        knowledge_news_embed = torch.mean(knowledge_news_embed, dim=2, keepdim=False)\n",
    "        \n",
    "        personalized_score = torch.matmul(knowledge_news_embed,user_embed.T)\n",
    "        \n",
    "        score1 =  self.aggregator_gate(time_aware_pop) \n",
    "        \n",
    "        personalized_score = torch.mean(personalized_score,dim=2,keepdim=False)\n",
    "        score2 =  (1-self.aggregator_gate(personalized_score))\n",
    "        \n",
    "        score = score1 + score2\n",
    "        return self.softmax(score)\n",
    "\n",
    "        \n",
    "class BPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPELoss, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, output, target): \n",
    "         \n",
    "         batch_size = target.shape[0]\n",
    "         total_no_samples = target.shape[1]\n",
    "         \n",
    "         \n",
    "         mask = target > 0\n",
    "         postive_index_select = torch.masked_select(output, mask)\n",
    "         \n",
    "         neg_mask = target == 0\n",
    "         negative_index_select = torch.masked_select(output, neg_mask)\n",
    "         negative_index_select = torch.reshape(negative_index_select,(batch_size,total_no_samples-1))\n",
    "         \n",
    "         negative_index_select,_ = torch.min(negative_index_select, dim=1, keepdim = True)\n",
    "         diff = torch.sub(postive_index_select, negative_index_select)\n",
    "         diff_sig = self.sigmoid(diff)\n",
    "         diff_log = torch.log(diff_sig)\n",
    "         return - torch.mean(diff_log)\n",
    "         \n",
    "\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer, train_dataloader,optimizer,model,loss_fn,device):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    \n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # Every data instance is an input + label pair\n",
    "        \n",
    "        inputs, labels = data\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        title = inputs[5]\n",
    "        entities = inputs[6]\n",
    "        ctr = inputs[7]\n",
    "        recency = inputs[8]\n",
    "        hist_title = inputs[0]\n",
    "        hist_popularity = inputs[2]\n",
    "        \n",
    "        title = torch.from_numpy(title)\n",
    "        entities = torch.from_numpy(entities)\n",
    "        ctr = torch.from_numpy(ctr)\n",
    "        recency = torch.from_numpy(recency)\n",
    "        hist_title = torch.from_numpy(hist_title)\n",
    "        hist_popularity = torch.from_numpy(hist_popularity)\n",
    "        labels = torch.from_numpy(labels)\n",
    "        \n",
    "        title = title.to(device)\n",
    "        entities = entities.to(device)\n",
    "        ctr = ctr.to(device)\n",
    "        recency = recency.to(device)\n",
    "        hist_title = hist_title.to(device)\n",
    "        hist_popularity = hist_popularity.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(title, entities, ctr, recency ,hist_title, hist_popularity )\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        print('  batch {} loss: {}'.format(i + 1, running_loss))\n",
    "        tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "        tb_writer.add_scalar('Loss/train', running_loss, tb_x)\n",
    "\n",
    "    return running_loss, i+1       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPRec DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Dataloader \"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from ebrec.utils._articles_behaviors import map_list_article_id_to_value\n",
    "from ebrec.utils._python import (\n",
    "    repeat_by_list_values_from_matrix\n",
    ")\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "\n",
    "def create_lookup_objects(\n",
    "    lookup_dictionary: dict[int, np.array], unknown_representation: str,is_array=True) -> tuple[dict[int, pl.Series], np.array]:\n",
    "    \"\"\"Creates lookup objects for efficient data retrieval.\n",
    "\n",
    "    This function generates a dictionary of indexes and a matrix from the given lookup dictionary.\n",
    "    The generated lookup matrix has an additional row based on the specified unknown representation\n",
    "    which could be either zeros or the mean of the values in the lookup dictionary.\n",
    "\n",
    "    Args:\n",
    "        lookup_dictionary (dict[int, np.array]): A dictionary where keys are unique identifiers (int)\n",
    "            and values are some representations which can be any data type, commonly used for lookup operations.\n",
    "        unknown_representation (str): Specifies the method to represent unknown entries.\n",
    "            It can be either 'zeros' to represent unknowns with a row of zeros, or 'mean' to represent\n",
    "            unknowns with a row of mean values computed from the lookup dictionary.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the unknown_representation is not either 'zeros' or 'mean',\n",
    "            a ValueError will be raised.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, pl.Series], np.array]: A tuple containing two items:\n",
    "            - A dictionary with the same keys as the lookup_dictionary where values are polars Series\n",
    "                objects containing a single value, which is the index of the key in the lookup dictionary.\n",
    "            - A numpy array where the rows correspond to the values in the lookup_dictionary and an\n",
    "                additional row representing unknown entries as specified by the unknown_representation argument.\n",
    "\n",
    "    Example:\n",
    "    >>> data = {\n",
    "            10: np.array([0.1, 0.2, 0.3]),\n",
    "            20: np.array([0.4, 0.5, 0.6]),\n",
    "            30: np.array([0.7, 0.8, 0.9]),\n",
    "        }\n",
    "    >>> lookup_dict, lookup_matrix = create_lookup_objects(data, \"zeros\")\n",
    "\n",
    "    >>> lookup_dict\n",
    "        {10: shape: (1,)\n",
    "            Series: '' [i64]\n",
    "            [\n",
    "                    1\n",
    "            ], 20: shape: (1,)\n",
    "            Series: '' [i64]\n",
    "            [\n",
    "                    2\n",
    "            ], 30: shape: (1,)\n",
    "            Series: '' [i64]\n",
    "            [\n",
    "                    3\n",
    "        ]}\n",
    "    >>> lookup_matrix\n",
    "        array([[0. , 0. , 0. ],\n",
    "            [0.1, 0.2, 0.3],\n",
    "            [0.4, 0.5, 0.6],\n",
    "            [0.7, 0.8, 0.9]])\n",
    "    \"\"\"\n",
    "    # MAKE LOOKUP DICTIONARY\n",
    "    lookup_indexes = {\n",
    "        id: pl.Series(\"\", [i]) for i, id in enumerate(lookup_dictionary, start=1)\n",
    "    }\n",
    "    # MAKE LOOKUP MATRIX\n",
    "    lookup_matrix = np.array(list(lookup_dictionary.values()))\n",
    "    if is_array:\n",
    "        if unknown_representation == \"zeros\":\n",
    "            UNKNOWN_ARRAY = np.zeros(lookup_matrix.shape[1], dtype=lookup_matrix.dtype)\n",
    "        elif unknown_representation == \"mean\":\n",
    "            UNKNOWN_ARRAY = np.mean(lookup_matrix, axis=0, dtype=lookup_matrix.dtype)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"'{unknown_representation}' is not a specified method. Can be either 'zeros' or 'mean'.\"\n",
    "            )\n",
    "\n",
    "        lookup_matrix = np.vstack([UNKNOWN_ARRAY, lookup_matrix])\n",
    "    return lookup_indexes, lookup_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NewsrecDataLoader(Dataset):\n",
    "    \"\"\"\n",
    "    A DataLoader for news recommendation.\n",
    "    \"\"\"\n",
    "\n",
    "    behaviors: pl.DataFrame\n",
    "    history_column: str\n",
    "    history_recency: str\n",
    "    inview_recency: str\n",
    "    article_dict: dict[int, any]\n",
    "    unknown_representation: str\n",
    "    eval_mode: bool = False\n",
    "    batch_size: int = 32\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL\n",
    "    labels_col: str = DEFAULT_LABELS_COL\n",
    "    user_col: str = DEFAULT_USER_COL\n",
    "    kwargs: field(default_factory=dict) = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Post-initialization method. Loads the data and sets additional attributes.\n",
    "        \"\"\"\n",
    "        self.lookup_article_index, self.lookup_article_matrix = create_lookup_objects(\n",
    "            self.article_dict, unknown_representation=self.unknown_representation\n",
    "        )\n",
    "        \n",
    "        self.unknown_index = [0]\n",
    "        self.X, self.y = self.load_data()\n",
    "        if self.kwargs is not None:\n",
    "            self.set_kwargs(self.kwargs)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(np.ceil(len(self.X) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self):\n",
    "        raise ValueError(\"Function '__getitem__' needs to be implemented.\")\n",
    "\n",
    "    def load_data(self) -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        \n",
    "        X = self.behaviors.drop(self.labels_col).with_columns(\n",
    "            pl.col(self.inview_col).list.len().alias(\"n_samples\")\n",
    "        )\n",
    "        y = self.behaviors[self.labels_col]\n",
    "        return X, y\n",
    "\n",
    "    def set_kwargs(self, kwargs: dict):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class PPRecDataLoader(NewsrecDataLoader):\n",
    "    \"\"\" PPRec DataLoader which inherits from the NewsrecDataLoader\"\"\"\n",
    "    entity_mapping: dict[int, list[int]] = None\n",
    "    ctr_mapping: dict[int, int] = None\n",
    "    popularity_mapping: dict[int, int] = None\n",
    "   \n",
    "    \n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.title_prefix = \"title_\"\n",
    "        self.entity_prefix = \"ner_clusters_text_\"\n",
    "        self.ctr_prefix = \"ctr_\"\n",
    "        self.pop_prefix = \"popularity_\"\n",
    "        \n",
    "        (\n",
    "            self.lookup_article_index_entity,\n",
    "            self.lookup_article_matrix_entity,\n",
    "        ) = create_lookup_objects(\n",
    "            self.entity_mapping, unknown_representation=self.unknown_representation\n",
    "        )\n",
    "\n",
    "        (\n",
    "            self.lookup_article_index_ctr,\n",
    "            self.lookup_article_matrix_ctr,\n",
    "        ) = create_lookup_objects(\n",
    "            self.ctr_mapping, unknown_representation=self.unknown_representation,is_array=False\n",
    "        )\n",
    "\n",
    "        (\n",
    "            self.lookup_article_index_pop,\n",
    "            self.lookup_article_matrix_pop,\n",
    "        ) = create_lookup_objects(\n",
    "            self.popularity_mapping, unknown_representation=self.unknown_representation,is_array=False\n",
    "        )\n",
    "\n",
    "        return super().__post_init__()\n",
    "\n",
    "    def transform(self, df: pl.DataFrame) -> tuple[pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Special case for NAML as it requires body-encoding, verticals, & subvertivals\n",
    "        \"\"\"\n",
    "        \n",
    "        title = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        \n",
    "        entities = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index_entity,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index_entity,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        ctr = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index_ctr,\n",
    "            fill_nulls=0,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index_ctr,\n",
    "            fill_nulls=0,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        popularity = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index_pop,\n",
    "            fill_nulls=0,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index_pop,\n",
    "            fill_nulls=0,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        \n",
    "        transformed_df =  (pl.DataFrame()\n",
    "            .with_columns(title.select(pl.all().name.prefix(self.title_prefix)))\n",
    "            .with_columns(entities.select(pl.all().name.prefix(self.entity_prefix)))\n",
    "            .with_columns(ctr.select(pl.all().name.prefix(self.ctr_prefix)))\n",
    "            .with_columns(popularity.select(pl.all().name.prefix(self.pop_prefix)))\n",
    "            )\n",
    "      \n",
    "        return transformed_df\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[tuple[np.ndarray], np.ndarray]:\n",
    "        batch_X = self.X[idx * self.batch_size : (idx + 1) * self.batch_size].pipe(\n",
    "            self.transform\n",
    "        )\n",
    "        \n",
    "        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        \n",
    "\n",
    "        if self.eval_mode:\n",
    "            \"\"\" Evaluation mode \"\"\"\n",
    "\n",
    "            batch_y = np.array(batch_y.to_list())\n",
    "            \n",
    "            his_input_title = np.array(\n",
    "                batch_X[self.title_prefix + self.history_column].to_list()\n",
    "            )\n",
    "            his_input_entity = np.array(\n",
    "                batch_X[self.entity_prefix + self.history_column].to_list()\n",
    "            )\n",
    "            his_input_ctr = np.array(\n",
    "                batch_X[self.ctr_prefix + self.history_column].to_list()\n",
    "            )\n",
    "            his_input_recency = np.array(\n",
    "                batch_X[self.title_prefix +self.history_recency].to_list()\n",
    "            )\n",
    "            his_input_pop = np.array(\n",
    "                batch_X[self.pop_prefix + self.history_column].to_list()\n",
    "            )\n",
    "\n",
    "            pred_input_title = np.array(\n",
    "                batch_X[self.title_prefix + self.inview_col].to_list()\n",
    "            )\n",
    "            \n",
    "            pred_input_entity = np.array(\n",
    "                batch_X[self.entity_prefix + self.inview_col].to_list()\n",
    "            )\n",
    "            pred_input_ctr = np.array(\n",
    "                batch_X[self.ctr_prefix + self.inview_col].to_list()\n",
    "            )\n",
    "            pred_input_recency = np.array(\n",
    "                batch_X[self.title_prefix + self.inview_recency].to_list()\n",
    "            )\n",
    "            pred_input_pop = np.array(\n",
    "                batch_X[self.pop_prefix + self.inview_col].to_list()\n",
    "            )\n",
    "            \n",
    "            pred_input_title = np.squeeze(\n",
    "                self.lookup_article_matrix[pred_input_title], axis=2\n",
    "            )\n",
    "            \n",
    "            pred_input_entity = np.squeeze(\n",
    "                self.lookup_article_matrix_entity[pred_input_entity], axis=2\n",
    "            )\n",
    "            \n",
    "            his_input_title = np.squeeze(\n",
    "                self.lookup_article_matrix[his_input_title], axis=2\n",
    "                )\n",
    "                \n",
    "            his_input_entity = np.squeeze(\n",
    "                    self.lookup_article_matrix_entity[his_input_entity], axis=2\n",
    "                    )\n",
    "            pred_input_ctr = np.squeeze(\n",
    "                    pred_input_ctr,axis=2\n",
    "                )\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        else:\n",
    "\n",
    "            \"\"\" Train mode \"\"\"\n",
    "            \n",
    "            batch_y = np.array(batch_y.to_list())\n",
    "            \n",
    "            his_input_title = np.array(\n",
    "                batch_X[self.title_prefix + self.history_column].to_list()\n",
    "            )\n",
    "            his_input_entity = np.array(\n",
    "                batch_X[self.entity_prefix + self.history_column].to_list()\n",
    "            )\n",
    "            his_input_ctr = np.array(\n",
    "                batch_X[self.ctr_prefix + self.history_column].to_list()\n",
    "            )\n",
    "            his_input_recency = np.array(\n",
    "                batch_X[self.title_prefix +self.history_recency].to_list()\n",
    "            )\n",
    "            his_input_pop = np.array(\n",
    "                batch_X[self.pop_prefix + self.history_column].to_list()\n",
    "            )\n",
    "            \n",
    "            pred_input_title = np.array(\n",
    "                batch_X[self.title_prefix + self.inview_col].to_list()\n",
    "            )\n",
    "           \n",
    "            pred_input_entity = np.array(\n",
    "                batch_X[self.entity_prefix + self.inview_col].to_list()\n",
    "            )\n",
    "            pred_input_ctr = np.array(\n",
    "                batch_X[self.ctr_prefix + self.inview_col].to_list()\n",
    "            )\n",
    "            pred_input_recency = np.array(\n",
    "                batch_X[self.title_prefix + self.inview_recency].to_list()\n",
    "            )\n",
    "            pred_input_pop = np.array(\n",
    "                batch_X[self.pop_prefix + self.inview_col].to_list()\n",
    "            )\n",
    "            \n",
    "            pred_input_title = np.squeeze(\n",
    "                self.lookup_article_matrix[pred_input_title], axis=2\n",
    "            )\n",
    "            \n",
    "            pred_input_entity = np.squeeze(\n",
    "                self.lookup_article_matrix_entity[pred_input_entity], axis=2\n",
    "            )\n",
    "             \n",
    "            his_input_title = np.squeeze(\n",
    "                self.lookup_article_matrix[his_input_title], axis=2\n",
    "                )\n",
    "                \n",
    "            his_input_entity = np.squeeze(\n",
    "                    self.lookup_article_matrix_entity[his_input_entity], axis=2\n",
    "                    )\n",
    "            pred_input_ctr = np.squeeze(\n",
    "                    pred_input_ctr,axis=2\n",
    "                )\n",
    "        \n",
    "        \n",
    "        final_X, final_Y =(\n",
    "            his_input_title,\n",
    "            his_input_entity,\n",
    "            his_input_ctr,\n",
    "            his_input_recency,\n",
    "            his_input_pop,\n",
    "            pred_input_title,\n",
    "            pred_input_entity,\n",
    "            pred_input_ctr,\n",
    "            pred_input_recency,\n",
    "            pred_input_pop\n",
    "        ), batch_y\n",
    "        \n",
    "        \n",
    "        \n",
    "        return final_X,final_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPRec Hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hparams_pprec:\n",
    "    title_size: int = 30\n",
    "    history_size: int = 50\n",
    "    body_size: int = 40\n",
    "    vert_num: int = 100\n",
    "    vert_emb_dim: int = 10\n",
    "    subvert_num: int = 100\n",
    "    subvert_emb_dim: int = 10\n",
    "    # MODEL ARCHITECTURE\n",
    "    dense_activation: str = \"relu\"\n",
    "    cnn_activation: str = \"relu\"\n",
    "    attention_hidden_dim: int = 200\n",
    "    filter_num: int = 400\n",
    "    window_size: int = 3\n",
    "    # MODEL OPTIMIZER:\n",
    "    optimizer: str = \"adam\"\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 0.0001\n",
    "    head_num: int = 8\n",
    "    embed_dim: int = 768\n",
    "    max_clicked: int = 10\n",
    "    m_size: int = 768\n",
    "    p_size: int = 768\n",
    "    weight_size: int = 256\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sohamchatterjee/miniforge3/envs/reco/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import torch \n",
    "import polars as pl\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ARTICLE_MODIFIED_TIMESTAMP_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers,concat_list_to_text\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sohamchatterjee/miniforge3/envs/reco/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Path to articles.parquet\"\"\"\n",
    "df_articles = pl.read_parquet(\"/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/articles.parquet\")\n",
    "\"\"\" Roberta Base is used as the transofrmer model for the doc embeddings\"\"\"\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping_title = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "article_mapping_title, article_mapping_entity, articles_ctr, popularity_mapping = {},{},{},{}\n",
    "with open('/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/demo_processed/article_mapping_title_DEMO.pkl', 'rb') as handle:\n",
    "    article_mapping_title = pickle.load(handle)\n",
    "with open('/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/demo_processed/article_mapping_entity_DEMO.pkl', 'rb') as handle:\n",
    "    article_mapping_entity = pickle.load(handle)\n",
    "with open('/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/demo_processed/articles_ctr_DEMO.pkl', 'rb') as handle:\n",
    "    articles_ctr = pickle.load(handle)\n",
    "with open('/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/demo_processed/popularity_mapping_DEMO.pkl', 'rb') as handle:\n",
    "    popularity_mapping = pickle.load(handle)\n",
    "\n",
    "COLUMNS = [\n",
    "   'user_id',\n",
    "   'article_id_fixed',\n",
    "   'article_ids_inview',\n",
    "   'article_ids_clicked',\n",
    "   'impression_id',\n",
    "   'labels',\n",
    "   'recency_inview',\n",
    "   'recency_hist'  \n",
    "]\n",
    "df_train  = pl.scan_parquet(\"/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/demo_processed/train_DEMO.parquet\").select(COLUMNS).collect()\n",
    "\n",
    "df_validation =  pl.scan_parquet(\"/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/demo_processed/val_DEMO.parquet\").select(COLUMNS).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just for sanity checking. Comment this out when running the traning pipeline\n",
    "df_train = df_train.head(10)\n",
    "df_validation = df_validation.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = PPRecDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping_title,\n",
    "    entity_mapping=article_mapping_entity,\n",
    "    ctr_mapping=articles_ctr,\n",
    "    popularity_mapping = popularity_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    history_recency = 'recency_hist',\n",
    "    inview_recency = 'recency_inview',\n",
    "    eval_mode=False,\n",
    "    batch_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = PPRecDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping_title,\n",
    "    entity_mapping=article_mapping_entity,\n",
    "    ctr_mapping=articles_ctr,\n",
    "    popularity_mapping = popularity_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    history_recency = 'recency_hist',\n",
    "    inview_recency = 'recency_inview',\n",
    "    eval_mode=True,\n",
    "    batch_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "  batch 1 loss: 0.6892244219779968\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "LOSS train 0.6892244219779968 valid 0.6577883958816528\n",
      "EPOCH 2:\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "  batch 1 loss: 0.6899724006652832\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "LOSS train 0.6899724006652832 valid 0.6553137302398682\n",
      "EPOCH 3:\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "  batch 1 loss: 0.6849033236503601\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "LOSS train 0.6849033236503601 valid 0.6535739898681641\n",
      "EPOCH 4:\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "  batch 1 loss: 0.678376317024231\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "LOSS train 0.678376317024231 valid 0.6480170488357544\n",
      "EPOCH 5:\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "  batch 1 loss: 0.6478434801101685\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "LOSS train 0.6478434801101685 valid 0.6489729285240173\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/pprec_check{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "loss_fn = BPELoss()\n",
    "model = PPRec(hparams_pprec,word2vec_embedding)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            lr=0.01,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    running_train_loss, total_no_batches = train_one_epoch(epoch_number, writer,train_dataloader, optimizer,model,loss_fn,device)\n",
    "    avg_loss = running_train_loss / total_no_batches\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vtitle = vinputs[5]\n",
    "            ventities = vinputs[6]\n",
    "            vctr = vinputs[7]\n",
    "            vrecency = vinputs[8]\n",
    "            vhist_title = vinputs[0]\n",
    "            vhist_popularity = vinputs[2]\n",
    "\n",
    "            vtitle = torch.from_numpy(vtitle)\n",
    "            ventities = torch.from_numpy(ventities)\n",
    "            vctr = torch.from_numpy(vctr)\n",
    "            vrecency = torch.from_numpy(vrecency)\n",
    "            vhist_title = torch.from_numpy(vhist_title)\n",
    "            vhist_popularity = torch.from_numpy(vhist_popularity)\n",
    "            vlabels = torch.from_numpy(vlabels)\n",
    "        \n",
    "            vtitle = vtitle.to(device)\n",
    "            ventities = ventities.to(device)\n",
    "            vctr = vctr.to(device)\n",
    "            vrecency = vrecency.to(device)\n",
    "            vhist_title = vhist_title.to(device)\n",
    "            vhist_popularity = vhist_popularity.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "            \n",
    "\n",
    "\n",
    "            voutputs = model(vtitle, ventities, vctr, vrecency , vhist_title, vhist_popularity )\n",
    "          \n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # EVAL MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sohamchatterjee/miniforge3/envs/reco/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "Reached here\n",
      "<MetricEvaluator class>: \n",
      " {\n",
      "    \"auc\": 0.475,\n",
      "    \"mrr\": 0.39499999999999996,\n",
      "    \"ndcg@5\": 0.5440741988597636,\n",
      "    \"ndcg@10\": 0.5440741988597636\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "import polars as pl\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_ARTICLE_MODIFIED_TIMESTAMP_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL\n",
    ")\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "\n",
    "saved_model = PPRec(hparams_pprec,word2vec_embedding)\n",
    "\n",
    "\"\"\" Best Model checkpoint path generated in the training loop \"\"\"\n",
    "PATH = '/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/model_20240628_214608_0'\n",
    "saved_model.load_state_dict(torch.load(PATH,weights_only=True,map_location=torch.device('cpu')))\n",
    "saved_model.to(device)\n",
    "import pickle\n",
    "article_mapping_title, article_mapping_entity, articles_ctr, popularity_mapping = {},{},{},{}\n",
    "\n",
    "\"\"\" Article title dictionary  \"\"\"\n",
    "with open('/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/demo_processed/article_mapping_title_DEMO.pkl', 'rb') as handle:\n",
    "    article_mapping_title = pickle.load(handle)\n",
    "    \"\"\" Article Entity dictionary  \"\"\"\n",
    "with open('/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/demo_processed/article_mapping_entity_DEMO.pkl', 'rb') as handle:\n",
    "    article_mapping_entity = pickle.load(handle)\n",
    "\"\"\" Article CTR dictionary  \"\"\"\n",
    "with open('/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/demo_processed/articles_ctr_DEMO.pkl', 'rb') as handle:\n",
    "    articles_ctr = pickle.load(handle)\n",
    "\"\"\" Article Popularitydictionary  \"\"\"\n",
    "with open('/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/demo_processed/popularity_mapping_DEMO.pkl', 'rb') as handle:\n",
    "    popularity_mapping = pickle.load(handle)\n",
    "\n",
    "\n",
    "COLUMNS = [\n",
    "   'user_id',\n",
    "   'article_id_fixed',\n",
    "   'article_ids_inview',\n",
    "   'article_ids_clicked',\n",
    "   'impression_id',\n",
    "   'labels',\n",
    "   'recency_inview',\n",
    "   'recency_hist'  \n",
    "]\n",
    "\n",
    "\"\"\" Demo validation parquet file\"\"\"\n",
    "df_validation =  pl.scan_parquet(\"/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/PP-Rec/DocumentedCode/demo_processed/val_DEMO.parquet\").collect()\n",
    "\n",
    "\"\"\" This is just for sanity checking\"\"\"\n",
    "df_validation = df_validation.head(10)\n",
    "\n",
    "val_dataloader = PPRecDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping_title,\n",
    "    entity_mapping=article_mapping_entity,\n",
    "    ctr_mapping=articles_ctr,\n",
    "    popularity_mapping = popularity_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    history_recency = 'recency_hist',\n",
    "    inview_recency = 'recency_inview',\n",
    "    eval_mode=True,\n",
    "    batch_size=1024,\n",
    ")\n",
    "\n",
    "\n",
    "saved_model.eval()\n",
    "\n",
    "predictions = np.empty(shape=(4,5))\n",
    "with torch.no_grad():\n",
    "    for i, vdata in enumerate(val_dataloader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vtitle = vinputs[5]\n",
    "            ventities = vinputs[6]\n",
    "            vctr = vinputs[7]\n",
    "            vrecency = vinputs[8]\n",
    "            vhist_title = vinputs[0]\n",
    "            vhist_popularity = vinputs[2]\n",
    "\n",
    "            vtitle = torch.from_numpy(vtitle)\n",
    "            ventities = torch.from_numpy(ventities)\n",
    "            vctr = torch.from_numpy(vctr)\n",
    "            vrecency = torch.from_numpy(vrecency)\n",
    "            vhist_title = torch.from_numpy(vhist_title)\n",
    "            vhist_popularity = torch.from_numpy(vhist_popularity)\n",
    "            vlabels = torch.from_numpy(vlabels)\n",
    "            \n",
    "            \n",
    "            \n",
    "            vtitle = vtitle.to(device)\n",
    "            ventities = ventities.to(device)\n",
    "            vctr = vctr.to(device)\n",
    "            vrecency = vrecency.to(device)\n",
    "            vhist_title = vhist_title.to(device)\n",
    "            vhist_popularity = vhist_popularity.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "            \n",
    "            \n",
    "\n",
    "            outputs = saved_model(vtitle, ventities, vctr, vrecency , vhist_title, vhist_popularity).cpu().detach().numpy()\n",
    "            predictions = np.concatenate([predictions,outputs],axis=0)\n",
    "            \n",
    "           \n",
    "            \n",
    "\n",
    "predictions = predictions[4:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_validation = df_validation.with_columns(pl.Series(name=\"predicted_scores\", values=predictions)) \n",
    "\n",
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"predicted_scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "print(metrics.evaluate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
