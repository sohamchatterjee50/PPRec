{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants #\n",
    "\n",
    "# BEHAVIORS\n",
    "DEFAULT_IMPRESSION_TIMESTAMP_COL = \"impression_time\"\n",
    "DEFAULT_SCROLL_PERCENTAGE_COL = \"scroll_percentage\"\n",
    "DEFAULT_CLICKED_ARTICLES_COL = \"article_ids_clicked\"\n",
    "DEFAULT_INVIEW_ARTICLES_COL = \"article_ids_inview\"\n",
    "DEFAULT_IMPRESSION_ID_COL = \"impression_id\"\n",
    "DEFAULT_IS_SUBSCRIBER_COL = \"is_subscriber\"\n",
    "DEFAULT_IS_SSO_USER_COL = \"is_sso_user\"\n",
    "DEFAULT_ARTICLE_ID_COL = \"article_id\"\n",
    "DEFAULT_SESSION_ID_COL = \"session_id\"\n",
    "DEFAULT_READ_TIME_COL = \"read_time\"\n",
    "DEFAULT_DEVICE_COL = \"device_type\"\n",
    "DEFAULT_POSTCODE_COL = \"postcode\"\n",
    "DEFAULT_GENDER_COL = \"gender\"\n",
    "DEFAULT_USER_COL = \"user_id\"\n",
    "DEFAULT_AGE_COL = \"age\"\n",
    "\n",
    "DEFAULT_NEXT_SCROLL_PERCENTAGE_COL = f\"next_{DEFAULT_SCROLL_PERCENTAGE_COL}\"\n",
    "DEFAULT_NEXT_READ_TIME_COL = f\"next_{DEFAULT_READ_TIME_COL}\"\n",
    "\n",
    "# ARTICLES\n",
    "DEFAULT_ARTICLE_MODIFIED_TIMESTAMP_COL = \"last_modified_time\"\n",
    "DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL = \"published_time\"\n",
    "DEFAULT_SENTIMENT_LABEL_COL = \"sentiment_label\"\n",
    "DEFAULT_SENTIMENT_SCORE_COL = \"sentiment_score\"\n",
    "DEFAULT_TOTAL_READ_TIME_COL = \"total_read_time\"\n",
    "DEFAULT_TOTAL_PAGEVIEWS_COL = \"total_pageviews\"\n",
    "DEFAULT_TOTAL_INVIEWS_COL = \"total_inviews\"\n",
    "DEFAULT_ARTICLE_TYPE_COL = \"article_type\"\n",
    "DEFAULT_CATEGORY_STR_COL = \"category_str\"\n",
    "DEFAULT_SUBCATEGORY_COL = \"subcategory\"\n",
    "DEFAULT_ENTITIES_COL = \"entity_groups\"\n",
    "DEFAULT_IMAGE_IDS_COL = \"image_ids\"\n",
    "DEFAULT_SUBTITLE_COL = \"subtitle\"\n",
    "DEFAULT_CATEGORY_COL = \"category\"\n",
    "DEFAULT_NER_COL = \"ner_clusters\"\n",
    "DEFAULT_PREMIUM_COL = \"premium\"\n",
    "DEFAULT_TOPICS_COL = \"topics\"\n",
    "DEFAULT_TITLE_COL = \"title\"\n",
    "DEFAULT_BODY_COL = \"body\"\n",
    "DEFAULT_URL_COL = \"url\"\n",
    "\n",
    "# HISTORY\n",
    "DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL = f\"{DEFAULT_IMPRESSION_TIMESTAMP_COL}_fixed\"\n",
    "DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL = f\"{DEFAULT_SCROLL_PERCENTAGE_COL}_fixed\"\n",
    "DEFAULT_HISTORY_ARTICLE_ID_COL = f\"{DEFAULT_ARTICLE_ID_COL}_fixed\"\n",
    "DEFAULT_HISTORY_READ_TIME_COL = f\"{DEFAULT_READ_TIME_COL}_fixed\"\n",
    "\n",
    "# CREATE\n",
    "DEFAULT_KNOWN_USER_COL = \"is_known_user\"\n",
    "DEFAULT_LABELS_COL = \"labels\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports #\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass, field\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass, field\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_objects(\n",
    "    lookup_dictionary: dict[int, np.array], unknown_representation: str\n",
    ") -> tuple[dict[int, pl.Series], np.array]:\n",
    "    \"\"\"Creates lookup objects for efficient data retrieval.\n",
    "\n",
    "    This function generates a dictionary of indexes and a matrix from the given lookup dictionary.\n",
    "    The generated lookup matrix has an additional row based on the specified unknown representation\n",
    "    which could be either zeros or the mean of the values in the lookup dictionary.\n",
    "\n",
    "    Args:\n",
    "        lookup_dictionary (dict[int, np.array]): A dictionary where keys are unique identifiers (int)\n",
    "            and values are some representations which can be any data type, commonly used for lookup operations.\n",
    "        unknown_representation (str): Specifies the method to represent unknown entries.\n",
    "            It can be either 'zeros' to represent unknowns with a row of zeros, or 'mean' to represent\n",
    "            unknowns with a row of mean values computed from the lookup dictionary.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the unknown_representation is not either 'zeros' or 'mean',\n",
    "            a ValueError will be raised.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, pl.Series], np.array]: A tuple containing two items:\n",
    "            - A dictionary with the same keys as the lookup_dictionary where values are polars Series\n",
    "                objects containing a single value, which is the index of the key in the lookup dictionary.\n",
    "            - A numpy array where the rows correspond to the values in the lookup_dictionary and an\n",
    "                additional row representing unknown entries as specified by the unknown_representation argument.\n",
    "\n",
    "    Example:\n",
    "    >>> data = {\n",
    "            10: np.array([0.1, 0.2, 0.3]),\n",
    "            20: np.array([0.4, 0.5, 0.6]),\n",
    "            30: np.array([0.7, 0.8, 0.9]),\n",
    "        }\n",
    "    >>> lookup_dict, lookup_matrix = create_lookup_objects(data, \"zeros\")\n",
    "\n",
    "    >>> lookup_dict\n",
    "        {10: shape: (1,)\n",
    "            Series: '' [i64]\n",
    "            [\n",
    "                    1\n",
    "            ], 20: shape: (1,)\n",
    "            Series: '' [i64]\n",
    "            [\n",
    "                    2\n",
    "            ], 30: shape: (1,)\n",
    "            Series: '' [i64]\n",
    "            [\n",
    "                    3\n",
    "        ]}\n",
    "    >>> lookup_matrix\n",
    "        array([[0. , 0. , 0. ],\n",
    "            [0.1, 0.2, 0.3],\n",
    "            [0.4, 0.5, 0.6],\n",
    "            [0.7, 0.8, 0.9]])\n",
    "    \"\"\"\n",
    "    # MAKE LOOKUP DICTIONARY\n",
    "    lookup_indexes = {\n",
    "        id: pl.Series(\"\", [i]) for i, id in enumerate(lookup_dictionary, start=1)\n",
    "    }\n",
    "    # MAKE LOOKUP MATRIX\n",
    "    lookup_matrix = np.array(list(lookup_dictionary.values()))\n",
    "\n",
    "    if unknown_representation == \"zeros\":\n",
    "        UNKNOWN_ARRAY = np.zeros(lookup_matrix.shape[1], dtype=lookup_matrix.dtype)\n",
    "    elif unknown_representation == \"mean\":\n",
    "        UNKNOWN_ARRAY = np.mean(lookup_matrix, axis=0, dtype=lookup_matrix.dtype)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"'{unknown_representation}' is not a specified method. Can be either 'zeros' or 'mean'.\"\n",
    "        )\n",
    "\n",
    "    lookup_matrix = np.vstack([UNKNOWN_ARRAY, lookup_matrix])\n",
    "    return lookup_indexes, lookup_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_name(existing_names: list[str], base_name: str = \"new_name\"):\n",
    "    \"\"\"\n",
    "    Generate a unique name based on a list of existing names.\n",
    "\n",
    "    Args:\n",
    "        existing_names (list of str): The list of existing names.\n",
    "        base_name (str): The base name to start with. Default is 'newName'.\n",
    "\n",
    "    Returns:\n",
    "        str: A unique name.\n",
    "    Example\n",
    "    >>> existing_names = ['name1', 'name2', 'newName', 'newName_1']\n",
    "    >>> generate_unique_name(existing_names, 'newName')\n",
    "        'newName_2'\n",
    "    \"\"\"\n",
    "    if base_name not in existing_names:\n",
    "        return base_name\n",
    "\n",
    "    suffix = 1\n",
    "    new_name = f\"{base_name}_{suffix}\"\n",
    "\n",
    "    while new_name in existing_names:\n",
    "        suffix += 1\n",
    "        new_name = f\"{base_name}_{suffix}\"\n",
    "\n",
    "    return new_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_list_article_id_to_value(\n",
    "    behaviors: pl.DataFrame,\n",
    "    behaviors_column: str,\n",
    "    mapping: dict[int, pl.Series],\n",
    "    drop_nulls: bool = False,\n",
    "    fill_nulls: any = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Maps the values of a column in a DataFrame `behaviors` containing article IDs to their corresponding values\n",
    "    in a column in another DataFrame `articles`. The mapping is performed using a dictionary constructed from\n",
    "    the two DataFrames. The resulting DataFrame has the same columns as `behaviors`, but with the article IDs\n",
    "    replaced by their corresponding values.\n",
    "\n",
    "    Args:\n",
    "        behaviors (pl.DataFrame): The DataFrame containing the column to be mapped.\n",
    "        behaviors_column (str): The name of the column to be mapped in `behaviors`.\n",
    "        mapping (dict[int, pl.Series]): A dictionary with article IDs as keys and corresponding values as values.\n",
    "            Note, 'replace' works a lot faster when values are of type pl.Series!\n",
    "        drop_nulls (bool): If `True`, any rows in the resulting DataFrame with null values will be dropped.\n",
    "            If `False` and `fill_nulls` is specified, null values in `behaviors_column` will be replaced with `fill_null`.\n",
    "        fill_nulls (Optional[any]): If specified, any null values in `behaviors_column` will be replaced with this value.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with the same columns as `behaviors`, but with the article IDs in\n",
    "            `behaviors_column` replaced by their corresponding values in `mapping`.\n",
    "\n",
    "    Example:\n",
    "    >>> behaviors = pl.DataFrame(\n",
    "            {\"user_id\": [1, 2, 3, 4, 5], \"article_ids\": [[\"A1\", \"A2\"], [\"A2\", \"A3\"], [\"A1\", \"A4\"], [\"A4\", \"A4\"], None]}\n",
    "        )\n",
    "    >>> articles = pl.DataFrame(\n",
    "            {\n",
    "                \"article_id\": [\"A1\", \"A2\", \"A3\"],\n",
    "                \"article_type\": [\"News\", \"Sports\", \"Entertainment\"],\n",
    "            }\n",
    "        )\n",
    "    >>> articles_dict = dict(zip(articles[\"article_id\"], articles[\"article_type\"]))\n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            fill_nulls=\"Unknown\",\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        ┌─────────┬─────────────────────────────┐\n",
    "        │ user_id ┆ article_ids                 │\n",
    "        │ ---     ┆ ---                         │\n",
    "        │ i64     ┆ list[str]                   │\n",
    "        ╞═════════╪═════════════════════════════╡\n",
    "        │ 1       ┆ [\"News\", \"Sports\"]          │\n",
    "        │ 2       ┆ [\"Sports\", \"Entertainment\"] │\n",
    "        │ 3       ┆ [\"News\", \"Unknown\"]         │\n",
    "        │ 4       ┆ [\"Unknown\", \"Unknown\"]      │\n",
    "        │ 5       ┆ [\"Unknown\"]                 │\n",
    "        └─────────┴─────────────────────────────┘\n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            drop_nulls=True,\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        ┌─────────┬─────────────────────────────┐\n",
    "        │ user_id ┆ article_ids                 │\n",
    "        │ ---     ┆ ---                         │\n",
    "        │ i64     ┆ list[str]                   │\n",
    "        ╞═════════╪═════════════════════════════╡\n",
    "        │ 1       ┆ [\"News\", \"Sports\"]          │\n",
    "        │ 2       ┆ [\"Sports\", \"Entertainment\"] │\n",
    "        │ 3       ┆ [\"News\"]                    │\n",
    "        │ 4       ┆ null                        │\n",
    "        │ 5       ┆ null                        │\n",
    "        └─────────┴─────────────────────────────┘\n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        ┌─────────┬─────────────────────────────┐\n",
    "        │ user_id ┆ article_ids                 │\n",
    "        │ ---     ┆ ---                         │\n",
    "        │ i64     ┆ list[str]                   │\n",
    "        ╞═════════╪═════════════════════════════╡\n",
    "        │ 1       ┆ [\"News\", \"Sports\"]          │\n",
    "        │ 2       ┆ [\"Sports\", \"Entertainment\"] │\n",
    "        │ 3       ┆ [\"News\", null]              │\n",
    "        │ 4       ┆ [null, null]                │\n",
    "        │ 5       ┆ [null]                      │\n",
    "        └─────────┴─────────────────────────────┘\n",
    "    \"\"\"\n",
    "   \n",
    "    GROUPBY_ID = generate_unique_name(behaviors.columns, \"_groupby_id\")\n",
    "    behaviors = behaviors.lazy().with_row_index(GROUPBY_ID)\n",
    "    # =>\n",
    "    select_column = (\n",
    "        behaviors.select(pl.col(GROUPBY_ID), pl.col(behaviors_column))\n",
    "        .explode(behaviors_column)\n",
    "        .with_columns(pl.col(behaviors_column).replace(mapping, default=None))\n",
    "        .collect()\n",
    "    )\n",
    "    \n",
    "    if drop_nulls:\n",
    "        \n",
    "        select_column = select_column.drop_nulls()\n",
    "    elif fill_nulls is not None:\n",
    "        print(fill_nulls)\n",
    "        print(pl.col(behaviors_column).fill_null(fill_nulls))\n",
    "\n",
    "        select_column = select_column.with_columns(\n",
    "            pl.col(behaviors_column).fill_null(fill_nulls)\n",
    "        )\n",
    "    \n",
    "    select_column = (\n",
    "        select_column.lazy().group_by(GROUPBY_ID).agg(behaviors_column).collect()\n",
    "    )\n",
    "    \n",
    "    #print(behaviors)\n",
    "    return (\n",
    "        behaviors.drop(behaviors_column)\n",
    "        .collect()\n",
    "        .join(select_column, on=GROUPBY_ID, how=\"left\")\n",
    "        .drop(GROUPBY_ID)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_by_list_values_from_matrix(\n",
    "    input_array: np.array,\n",
    "    matrix: np.array,\n",
    "    repeats: np.array,\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    Example:\n",
    "        >>> input = np.array([[1, 0], [0, 0]])\n",
    "        >>> matrix = np.array([[7,8,9], [10,11,12]])\n",
    "        >>> repeats = np.array([1, 2])\n",
    "        >>> repeat_by_list_values_from_matrix(input, matrix, repeats)\n",
    "            array([[[10, 11, 12],\n",
    "                    [ 7,  8,  9]],\n",
    "                    [[ 7,  8,  9],\n",
    "                    [ 7,  8,  9]],\n",
    "                    [[ 7,  8,  9],\n",
    "                    [ 7,  8,  9]]])\n",
    "    \"\"\"\n",
    "    return np.repeat(matrix[input_array], repeats=repeats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_columns_in_df(df: pl.DataFrame, columns: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Checks whether all specified columns are present in a Polars DataFrame.\n",
    "    Raises a ValueError if any of the specified columns are not present in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame.\n",
    "        columns (list[str]): The names of the columns to check for.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    Examples:\n",
    "    >>> df = pl.DataFrame({\"user_id\": [1], \"first_name\": [\"J\"]})\n",
    "    >>> check_columns_in_df(df, columns=[\"user_id\", \"not_in\"])\n",
    "        ValueError: Invalid input provided. The dataframe does not contain columns ['not_in'].\n",
    "    \"\"\"\n",
    "    columns_not_in_df = [col for col in columns if col not in df.columns]\n",
    "    if columns_not_in_df:\n",
    "        raise ValueError(\n",
    "            f\"Invalid input provided. The DataFrame does not contain columns {columns_not_in_df}.\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_rows(df: pl.DataFrame, seed: int = None) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Shuffle the rows of a DataFrame. This methods allows for LazyFrame,\n",
    "    whereas, 'df.sample(fraction=1)' is not compatible.\n",
    "\n",
    "    Examples:\n",
    "    >>> df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [1, 2, 3], \"c\": [1, 2, 3]})\n",
    "    >>> shuffle_rows(df.lazy(), seed=123).collect()\n",
    "        shape: (3, 3)\n",
    "        ┌─────┬─────┬─────┐\n",
    "        │ a   ┆ b   ┆ c   │\n",
    "        │ --- ┆ --- ┆ --- │\n",
    "        │ i64 ┆ i64 ┆ i64 │\n",
    "        ╞═════╪═════╪═════╡\n",
    "        │ 1   ┆ 1   ┆ 1   │\n",
    "        │ 3   ┆ 3   ┆ 3   │\n",
    "        │ 2   ┆ 2   ┆ 2   │\n",
    "        └─────┴─────┴─────┘\n",
    "    >>> shuffle_rows(df.lazy(), seed=None).collect().sort(\"a\")\n",
    "        shape: (3, 3)\n",
    "        ┌─────┬─────┬─────┐\n",
    "        │ a   ┆ b   ┆ c   │\n",
    "        │ --- ┆ --- ┆ --- │\n",
    "        │ i64 ┆ i64 ┆ i64 │\n",
    "        ╞═════╪═════╪═════╡\n",
    "        │ 1   ┆ 1   ┆ 1   │\n",
    "        │ 2   ┆ 2   ┆ 2   │\n",
    "        │ 3   ┆ 3   ┆ 3   │\n",
    "        └─────┴─────┴─────┘\n",
    "\n",
    "    Test_:\n",
    "    >>> all([sum(row) == row[0]*3 for row in shuffle_rows(df, seed=None).iter_rows()])\n",
    "        True\n",
    "\n",
    "    Note:\n",
    "        Be aware that 'pl.all().shuffle()' shuffles columns-wise, i.e., with if pl.all().shuffle(None)\n",
    "        each column's element are shuffled independently from each other (example might change with no seed):\n",
    "    >>> df_ = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [1, 2, 3], \"c\": [1, 2, 3]}).select(pl.all().shuffle(None)).sort(\"a\")\n",
    "    >>> df_\n",
    "        shape: (3, 3)\n",
    "        ┌─────┬─────┬─────┐\n",
    "        │ a   ┆ b   ┆ c   │\n",
    "        │ --- ┆ --- ┆ --- │\n",
    "        │ i64 ┆ i64 ┆ i64 │\n",
    "        ╞═════╪═════╪═════╡\n",
    "        │ 1   ┆ 3   ┆ 1   │\n",
    "        │ 2   ┆ 2   ┆ 3   │\n",
    "        │ 3   ┆ 1   ┆ 2   │\n",
    "        └─────┴─────┴─────┘\n",
    "    >>> all([sum(row) == row[0]*3 for row in shuffle_rows(df_, seed=None).iter_rows()])\n",
    "        False\n",
    "    \"\"\"\n",
    "    seed = seed if seed is not None else random.randint(1, 1_000_000)\n",
    "    return df.select(pl.all().shuffle(seed))\n",
    "\n",
    "\n",
    "\n",
    "def shuffle_list_column(\n",
    "    df: pl.DataFrame, column: str, seed: int = None\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Shuffles the values in a list column of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame.\n",
    "        column (str): The name of the column to shuffle.\n",
    "        seed (int, optional): An optional seed value.\n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with the specified column shuffled.\n",
    "\n",
    "    Example:\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                \"id\": [1, 2, 3],\n",
    "                \"list_col\": [[\"a-\", \"b-\", \"c-\"], [\"a#\", \"b#\"], [\"a@\", \"b@\", \"c@\"]],\n",
    "                \"rdn\": [\"h\", \"e\", \"y\"],\n",
    "            }\n",
    "        )\n",
    "    >>> shuffle_list_column(df, 'list_col', seed=1)\n",
    "        shape: (3, 3)\n",
    "        ┌─────┬────────────────────┬─────┐\n",
    "        │ id  ┆ list_col           ┆ rdn │\n",
    "        │ --- ┆ ---                ┆ --- │\n",
    "        │ i64 ┆ list[str]          ┆ str │\n",
    "        ╞═════╪════════════════════╪═════╡\n",
    "        │ 1   ┆ [\"c-\", \"b-\", \"a-\"] ┆ h   │\n",
    "        │ 2   ┆ [\"a#\", \"b#\"]       ┆ e   │\n",
    "        │ 3   ┆ [\"b@\", \"c@\", \"a@\"] ┆ y   │\n",
    "        └─────┴────────────────────┴─────┘\n",
    "\n",
    "    No seed:\n",
    "    >>> shuffle_list_column(df, 'list_col', seed=None)\n",
    "        shape: (3, 3)\n",
    "        ┌─────┬────────────────────┬─────┐\n",
    "        │ id  ┆ list_col           ┆ rdn │\n",
    "        │ --- ┆ ---                ┆ --- │\n",
    "        │ i64 ┆ list[str]          ┆ str │\n",
    "        ╞═════╪════════════════════╪═════╡\n",
    "        │ 1   ┆ [\"b-\", \"a-\", \"c-\"] ┆ h   │\n",
    "        │ 2   ┆ [\"a#\", \"b#\"]       ┆ e   │\n",
    "        │ 3   ┆ [\"a@\", \"c@\", \"b@\"] ┆ y   │\n",
    "        └─────┴────────────────────┴─────┘\n",
    "\n",
    "    Test_:\n",
    "    >>> assert (\n",
    "            sorted(shuffle_list_column(df, \"list_col\", seed=None)[\"list_col\"].to_list()[0])\n",
    "            == df[\"list_col\"].to_list()[0]\n",
    "        )\n",
    "\n",
    "    >>> df = pl.DataFrame({\n",
    "            'id': [1, 2, 3],\n",
    "            'list_col': [[6, 7, 8], [-6, -7, -8], [60, 70, 80]],\n",
    "            'rdn': ['h', 'e', 'y']\n",
    "        })\n",
    "    >>> shuffle_list_column(df.lazy(), 'list_col', seed=2).collect()\n",
    "        shape: (3, 3)\n",
    "        ┌─────┬──────────────┬─────┐\n",
    "        │ id  ┆ list_col     ┆ rdn │\n",
    "        │ --- ┆ ---          ┆ --- │\n",
    "        │ i64 ┆ list[i64]    ┆ str │\n",
    "        ╞═════╪══════════════╪═════╡\n",
    "        │ 1   ┆ [7, 6, 8]    ┆ h   │\n",
    "        │ 2   ┆ [-8, -7, -6] ┆ e   │\n",
    "        │ 3   ┆ [60, 80, 70] ┆ y   │\n",
    "        └─────┴──────────────┴─────┘\n",
    "\n",
    "    Test_:\n",
    "    >>> assert (\n",
    "            sorted(shuffle_list_column(df, \"list_col\", seed=None)[\"list_col\"].to_list()[0])\n",
    "            == df[\"list_col\"].to_list()[0]\n",
    "        )\n",
    "    \"\"\"\n",
    "    _COLUMN_ORDER = df.columns\n",
    "    GROUPBY_ID = generate_unique_name(_COLUMN_ORDER, \"_groupby_id\")\n",
    "\n",
    "    # Row count is\n",
    "    df = df.with_row_index(GROUPBY_ID)\n",
    "    df_shuffle = (\n",
    "        df.explode(column)\n",
    "        .pipe(shuffle_rows, seed=seed)\n",
    "        .group_by(GROUPBY_ID)\n",
    "        .agg(column)\n",
    "    )\n",
    "    return (\n",
    "        df.drop(column)\n",
    "        .join(df_shuffle, on=GROUPBY_ID, how=\"left\")\n",
    "        .drop(GROUPBY_ID)\n",
    "        .select(_COLUMN_ORDER)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def create_binary_labels_column(\n",
    "    df: pl.DataFrame,\n",
    "    shuffle: bool = True,\n",
    "    seed: int = None,\n",
    "    clicked_col: str = DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    label_col: str = DEFAULT_LABELS_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Creates a new column in a DataFrame containing binary labels indicating\n",
    "    whether each article ID in the \"article_ids\" column is present in the corresponding\n",
    "    \"list_destination\" column.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with an additional \"labels\" column.\n",
    "\n",
    "    Examples:\n",
    "    >>> from ebrec.utils._constants import (\n",
    "            DEFAULT_CLICKED_ARTICLES_COL,\n",
    "            DEFAULT_INVIEW_ARTICLES_COL,\n",
    "            DEFAULT_LABELS_COL,\n",
    "        )\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                DEFAULT_INVIEW_ARTICLES_COL: [[1, 2, 3], [4, 5, 6], [7, 8]],\n",
    "                DEFAULT_CLICKED_ARTICLES_COL: [[2, 3, 4], [3, 5], None],\n",
    "            }\n",
    "        )\n",
    "    >>> create_binary_labels_column(df)\n",
    "        shape: (3, 3)\n",
    "        ┌────────────────────┬─────────────────────┬───────────┐\n",
    "        │ article_ids_inview ┆ article_ids_clicked ┆ labels    │\n",
    "        │ ---                ┆ ---                 ┆ ---       │\n",
    "        │ list[i64]          ┆ list[i64]           ┆ list[i8]  │\n",
    "        ╞════════════════════╪═════════════════════╪═══════════╡\n",
    "        │ [1, 2, 3]          ┆ [2, 3, 4]           ┆ [0, 1, 1] │\n",
    "        │ [4, 5, 6]          ┆ [3, 5]              ┆ [0, 1, 0] │\n",
    "        │ [7, 8]             ┆ null                ┆ [0, 0]    │\n",
    "        └────────────────────┴─────────────────────┴───────────┘\n",
    "    >>> create_binary_labels_column(df.lazy(), shuffle=True, seed=123).collect()\n",
    "        shape: (3, 3)\n",
    "        ┌────────────────────┬─────────────────────┬───────────┐\n",
    "        │ article_ids_inview ┆ article_ids_clicked ┆ labels    │\n",
    "        │ ---                ┆ ---                 ┆ ---       │\n",
    "        │ list[i64]          ┆ list[i64]           ┆ list[i8]  │\n",
    "        ╞════════════════════╪═════════════════════╪═══════════╡\n",
    "        │ [3, 1, 2]          ┆ [2, 3, 4]           ┆ [1, 0, 1] │\n",
    "        │ [5, 6, 4]          ┆ [3, 5]              ┆ [1, 0, 0] │\n",
    "        │ [7, 8]             ┆ null                ┆ [0, 0]    │\n",
    "        └────────────────────┴─────────────────────┴───────────┘\n",
    "    Test_:\n",
    "    >>> assert create_binary_labels_column(df, shuffle=False)[DEFAULT_LABELS_COL].to_list() == [\n",
    "            [0, 1, 1],\n",
    "            [0, 1, 0],\n",
    "            [0, 0],\n",
    "        ]\n",
    "    >>> assert create_binary_labels_column(df, shuffle=True)[DEFAULT_LABELS_COL].list.sum().to_list() == [\n",
    "            2,\n",
    "            1,\n",
    "            0,\n",
    "        ]\n",
    "    \"\"\"\n",
    "    _check_columns_in_df(df, [inview_col, clicked_col])\n",
    "    _COLUMNS = df.columns\n",
    "    GROUPBY_ID = generate_unique_name(_COLUMNS, \"_groupby_id\")\n",
    "\n",
    "    df = df.with_row_index(GROUPBY_ID)\n",
    "\n",
    "    if shuffle:\n",
    "        df = shuffle_list_column(df, column=inview_col, seed=seed)\n",
    "\n",
    "    df_labels = (\n",
    "        df.explode(inview_col)\n",
    "        .with_columns(\n",
    "            pl.col(inview_col).is_in(pl.col(clicked_col)).cast(pl.Int8).alias(label_col)\n",
    "        )\n",
    "        .group_by(GROUPBY_ID)\n",
    "        .agg(label_col)\n",
    "    )\n",
    "    return (\n",
    "        df.join(df_labels, on=GROUPBY_ID, how=\"left\")\n",
    "        .drop(GROUPBY_ID)\n",
    "        .select(_COLUMNS + [label_col])\n",
    "    )\n",
    "\n",
    "def create_lookup_dict(df: pl.DataFrame, key: str, value: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a dictionary lookup table from a Pandas-like DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The DataFrame from which to create the lookup table.\n",
    "        key (str): The name of the column containing the keys for the lookup table.\n",
    "        value (str): The name of the column containing the values for the lookup table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the values from the `key` column of the DataFrame\n",
    "            and the values are the values from the `value` column of the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> df = pl.DataFrame({'id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})\n",
    "        >>> create_lookup_dict(df, 'id', 'name')\n",
    "            {1: 'Alice', 2: 'Bob', 3: 'Charlie'}\n",
    "    \"\"\"\n",
    "    return dict(zip(df[key], df[value]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_article_id_to_value_mapping(\n",
    "    df: pl.DataFrame,\n",
    "    value_col: str,\n",
    "    article_col: str = DEFAULT_ARTICLE_ID_COL,\n",
    "):\n",
    "    return create_lookup_dict(\n",
    "        df.select(article_col, value_col), key=article_col, value=value_col\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_id_to_int_mapping(\n",
    "    df: pl.DataFrame, user_col: str = DEFAULT_USER_COL, value_str: str = \"id\"\n",
    "):\n",
    "    return create_lookup_dict(\n",
    "        df.select(pl.col(user_col).unique()).with_row_index(value_str),\n",
    "        key=user_col,\n",
    "        value=value_str,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA:\n",
    "\n",
    "TOKEN_COL = \"tokens\"\n",
    "N_SAMPLES = \"n\"\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "\n",
    "PATH_DATA = Path(\"/Users/sohamchatterjee/Documents/UvA/RecSYS/Project/ebnerd_data\")\n",
    "df_articles = (\n",
    "    pl.scan_parquet(PATH_DATA.joinpath(\"articles.parquet\"))\n",
    "    .select(pl.col(DEFAULT_ARTICLE_ID_COL, DEFAULT_CATEGORY_COL))\n",
    "    .with_columns(pl.Series(TOKEN_COL, np.random.randint(0, 20, (1, 10))))\n",
    "    .collect()\n",
    ")\n",
    "df_history = (\n",
    "    pl.scan_parquet(PATH_DATA.joinpath(\"ebnerd_demo\",\"train\", \"history.parquet\"))\n",
    "    .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "    .with_columns(pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL).list.tail(3))\n",
    ")\n",
    "df_behaviors = (\n",
    "    pl.scan_parquet(PATH_DATA.joinpath(\"ebnerd_demo\",\"train\", \"behaviors.parquet\"))\n",
    "    .select(DEFAULT_USER_COL, DEFAULT_INVIEW_ARTICLES_COL, DEFAULT_CLICKED_ARTICLES_COL)\n",
    "    .with_columns(pl.col(DEFAULT_INVIEW_ARTICLES_COL).list.len().alias(N_SAMPLES))\n",
    "    .join(df_history, on=DEFAULT_USER_COL, how=\"left\")\n",
    "    .collect()\n",
    "    .pipe(create_binary_labels_column)\n",
    ")\n",
    "# => MAPPINGS:\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=TOKEN_COL\n",
    ")\n",
    "user_mapping = create_user_id_to_int_mapping(df=df_behaviors)\n",
    "# => NPRATIO IMPRESSION - SAME LENGTHS:\n",
    "df_behaviors_train = df_behaviors.filter(pl.col(N_SAMPLES) == pl.col(N_SAMPLES).min())\n",
    "# => FOR TEST-DATALOADER\n",
    "label_lengths = df_behaviors[DEFAULT_INVIEW_ARTICLES_COL].list.len().to_list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9803607"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles['article_id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPRecDataLoader(Dataset):\n",
    "    \"\"\"\n",
    "    NPA and LSTUR shares the same DataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,behaviors: pd.DataFrame, history_column: str, article_dict: dict[int, any], unknown_representation: str,\n",
    "                 eval_mode: bool = False,\n",
    "    batch_size: int = 32,\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    labels_col: str = DEFAULT_LABELS_COL,\n",
    "    user_col: str = DEFAULT_USER_COL, user_id_mapping: dict[int, int] = None, unknown_user_value: int = 0,kwargs: dict = field(default_factory=dict)):\n",
    "        self.behaviors = behaviors\n",
    "        self.history_column = history_column\n",
    "        self.article_dict = article_dict\n",
    "        self.unknown_representation = unknown_representation\n",
    "        self.eval_mode: bool = False\n",
    "        self.batch_size = batch_size\n",
    "        self.inview_col = inview_col \n",
    "        self.labels_col = labels_col\n",
    "        self.user_col = user_col \n",
    "        self.kwargs = kwargs\n",
    "        self.lookup_article_index, self.lookup_article_matrix = create_lookup_objects(\n",
    "            self.article_dict, unknown_representation=self.unknown_representation\n",
    "        )\n",
    "        #self.unknown_index = torch.tensor([0])\n",
    "        self.unknown_index = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.user_id_mapping = user_id_mapping\n",
    "        self.unknown_user_value = unknown_user_value\n",
    "\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return int(np.ceil(len(self.X) / float(self.batch_size)))\n",
    "    def set_kwargs(self, kwargs: dict):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "    \n",
    "    def load_data(self): \n",
    "    # -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \n",
    "        #print(self.behaviors)\n",
    "     \n",
    "        X = self.behaviors.drop(self.labels_col)\n",
    "       \n",
    "        X= X.with_columns(\n",
    "            n_samples=self.behaviors[self.inview_col].map_elements(len)\n",
    "        )\n",
    "       \n",
    "        y = self.behaviors[self.labels_col]\n",
    "       \n",
    "        self.X, self.y = X,y\n",
    "        return self.X, self.y\n",
    "\n",
    "    \n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        #print(df)\n",
    "        \n",
    "        return (\n",
    "            df.pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=self.history_column,\n",
    "                mapping=self.lookup_article_index,\n",
    "                fill_nulls=self.unknown_index,\n",
    "                drop_nulls=False,\n",
    "            )\n",
    "            .pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=self.inview_col,\n",
    "                mapping=self.lookup_article_index,\n",
    "                fill_nulls=self.unknown_index,\n",
    "                drop_nulls=False,\n",
    "            )\n",
    "            .with_columns(\n",
    "                **{self.user_col: lambda x: x[self.user_col].replace(\n",
    "                    self.user_id_mapping, default=self.unknown_user_value\n",
    "                )}\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        user_indexes:       ()\n",
    "        his_input_title:    (samples, history_size, document_dimension)\n",
    "        pred_input_title:   (samples, npratio, document_dimension)\n",
    "        batch_y:            (samples, npratio)\n",
    "        \"\"\"\n",
    "        batch_X1 = self.X[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        #print(batch_X1)\n",
    "        batch_X = batch_X1.pipe(\n",
    "            self.transform\n",
    "        )\n",
    "        \n",
    "        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        # =>\n",
    "        if self.eval_mode:\n",
    "            repeats = np.array(batch_X[\"n_samples\"])\n",
    "            # =>\n",
    "            batch_y = np.array(batch_y.explode().to_list()).reshape(-1, 1)\n",
    "            # =>\n",
    "            user_indexes = np.array(\n",
    "                batch_X[self.user_col].repeat(repeats).explode()\n",
    "            ).reshape(-1, 1)\n",
    "            # =>\n",
    "            his_input_title = repeat_by_list_values_from_matrix(\n",
    "                batch_X[self.history_column].to_list(),\n",
    "                matrix=self.lookup_article_matrix,\n",
    "                repeats=repeats,\n",
    "            )\n",
    "            # =>\n",
    "            pred_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.inview_col].explode().to_list()\n",
    "            ]\n",
    "        else:\n",
    "            # =>\n",
    "            batch_y = np.array(batch_y.to_list())\n",
    "            # =>\n",
    "            user_indexes = np.array(batch_X[self.user_col].to_list()).reshape(-1, 1)\n",
    "            # =>\n",
    "            his_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.history_column].to_list()\n",
    "            ]\n",
    "            # =>\n",
    "            print(batch_X[self.inview_col].to_list())\n",
    "            pred_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.inview_col].to_list()\n",
    "            ]\n",
    "            \n",
    "            pred_input_title = np.squeeze(pred_input_title, axis=2)\n",
    "        # =>\n",
    "        his_input_title = np.squeeze(his_input_title, axis=2)\n",
    "        return (\n",
    "            torch.from_numpy(user_indexes).float(),\n",
    "            torch.from_numpy(his_input_title).float(),\n",
    "            torch.from_numpy(pred_input_title).float(),\n",
    "        ), torch.from_numpy(batch_y).float()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for idx in range(len(self)):\n",
    "            yield self.__getitem__(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_PPRecDataLoader():\n",
    "    train_dataloader = PPRecDataLoader(\n",
    "        behaviors=df_behaviors_train,\n",
    "        article_dict=article_mapping,\n",
    "        user_id_mapping=user_mapping,\n",
    "        history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "        unknown_representation=\"zeros\",\n",
    "        batch_size=4,\n",
    "    )\n",
    "    train_dataloader.load_data()\n",
    "   \n",
    "\n",
    "    batch = train_dataloader.__iter__().__next__()\n",
    "\n",
    "    assert train_dataloader.__len__() == int(np.ceil(df_behaviors_train.shape[0] / 100))\n",
    "    assert len(batch) == 2, \"There should be two outputs: (inputs, labels)\"\n",
    "    assert (\n",
    "        len(batch[0]) == 3\n",
    "    ), \"LSTUR has two outputs (user_indexes, his_input_title, pred_input_title_one)\"\n",
    "\n",
    "    for type_in_batch in batch[0][0]:\n",
    "        assert isinstance(\n",
    "            type_in_batch.ravel()[0], np.integer\n",
    "        ), \"Expected output to be integer; used for lookup value\"\n",
    "\n",
    "    assert isinstance(\n",
    "        batch[1].ravel()[0], np.integer\n",
    "    ), \"Expected output to be integer; this is label\"\n",
    "\n",
    "    test_dataloader = PPRecDataLoader(\n",
    "        user_id_mapping=user_mapping,\n",
    "        behaviors=df_behaviors,\n",
    "        article_dict=article_mapping,\n",
    "        history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "        unknown_representation=\"zeros\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        eval_mode=True,\n",
    "    )\n",
    "\n",
    "    batch = test_dataloader.__iter__().__next__()\n",
    "    assert len(batch[1]) == sum(\n",
    "        label_lengths[:BATCH_SIZE]\n",
    "    ), \"Should have unfolded all the test samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "col(\"article_id_fixed\").fill_null([0])\n",
      "0\n",
      "col(\"article_ids_inview\").fill_null([0])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "invalid literal value: '<function PPRecDataLoader.transform.<locals>.<lambda> at 0x2d213bec0>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[491], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_PPRecDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[490], line 13\u001b[0m, in \u001b[0;36mtest_PPRecDataLoader\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m PPRecDataLoader(\n\u001b[1;32m      3\u001b[0m     behaviors\u001b[38;5;241m=\u001b[39mdf_behaviors_train,\n\u001b[1;32m      4\u001b[0m     article_dict\u001b[38;5;241m=\u001b[39marticle_mapping,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m train_dataloader\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m---> 13\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_dataloader\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(df_behaviors_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere should be two outputs: (inputs, labels)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[489], line 142\u001b[0m, in \u001b[0;36mPPRecDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[489], line 92\u001b[0m, in \u001b[0;36mPPRecDataLoader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     90\u001b[0m batch_X1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#print(batch_X1)\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m batch_X \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_X1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size : (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# =>\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/reco/lib/python3.11/site-packages/polars/dataframe/frame.py:5229\u001b[0m, in \u001b[0;36mDataFrame.pipe\u001b[0;34m(self, function, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(\n\u001b[1;32m   5165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5166\u001b[0m     function: Callable[Concatenate[DataFrame, P], T],\n\u001b[1;32m   5167\u001b[0m     \u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m   5168\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[1;32m   5169\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   5170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5171\u001b[0m \u001b[38;5;124;03m    Offers a structured way to apply a sequence of user-defined functions (UDFs).\u001b[39;00m\n\u001b[1;32m   5172\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5227\u001b[0m \u001b[38;5;124;03m    └─────┴─────┘\u001b[39;00m\n\u001b[1;32m   5228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[489], line 62\u001b[0m, in \u001b[0;36mPPRecDataLoader.transform\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m#print(df)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_list_article_id_to_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbehaviors_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup_article_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_nulls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munknown_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_nulls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_list_article_id_to_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbehaviors_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minview_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup_article_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_nulls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munknown_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_nulls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_col\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_id_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munknown_user_value\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/reco/lib/python3.11/site-packages/polars/dataframe/frame.py:8301\u001b[0m, in \u001b[0;36mDataFrame.with_columns\u001b[0;34m(self, *exprs, **named_exprs)\u001b[0m\n\u001b[1;32m   8155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_columns\u001b[39m(\n\u001b[1;32m   8156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   8157\u001b[0m     \u001b[38;5;241m*\u001b[39mexprs: IntoExpr \u001b[38;5;241m|\u001b[39m Iterable[IntoExpr],\n\u001b[1;32m   8158\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_exprs: IntoExpr,\n\u001b[1;32m   8159\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   8160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   8161\u001b[0m \u001b[38;5;124;03m    Add columns to this DataFrame.\u001b[39;00m\n\u001b[1;32m   8162\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8299\u001b[0m \u001b[38;5;124;03m    └─────┴──────┴─────────────┘\u001b[39;00m\n\u001b[1;32m   8300\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 8301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcollect(_eager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/reco/lib/python3.11/site-packages/polars/lazyframe/frame.py:4256\u001b[0m, in \u001b[0;36mLazyFrame.with_columns\u001b[0;34m(self, *exprs, **named_exprs)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;124;03mAdd columns to this LazyFrame.\u001b[39;00m\n\u001b[1;32m   4115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4252\u001b[0m \u001b[38;5;124;03m└─────┴──────┴─────────────┘\u001b[39;00m\n\u001b[1;32m   4253\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4254\u001b[0m structify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOLARS_AUTO_STRUCTIFY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m-> 4256\u001b[0m pyexprs \u001b[38;5;241m=\u001b[39m \u001b[43mparse_as_list_of_expressions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4257\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m__structify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructify\u001b[49m\n\u001b[1;32m   4258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pyldf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ldf\u001b[38;5;241m.\u001b[39mwith_columns(pyexprs))\n",
      "File \u001b[0;32m~/miniforge3/envs/reco/lib/python3.11/site-packages/polars/utils/_parse_expr_input.py:45\u001b[0m, in \u001b[0;36mparse_as_list_of_expressions\u001b[0;34m(__structify, *inputs, **named_inputs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m named_inputs:\n\u001b[1;32m     44\u001b[0m     named_exprs \u001b[38;5;241m=\u001b[39m _parse_named_inputs(named_inputs, structify\u001b[38;5;241m=\u001b[39m__structify)\n\u001b[0;32m---> 45\u001b[0m     exprs\u001b[38;5;241m.\u001b[39mextend(named_exprs)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exprs\n",
      "File \u001b[0;32m~/miniforge3/envs/reco/lib/python3.11/site-packages/polars/utils/_parse_expr_input.py:82\u001b[0m, in \u001b[0;36m_parse_named_inputs\u001b[0;34m(named_inputs, structify)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_named_inputs\u001b[39m(\n\u001b[1;32m     79\u001b[0m     named_inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, IntoExpr], \u001b[38;5;241m*\u001b[39m, structify: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     80\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[PyExpr]:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m named_inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mparse_as_expression\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructify\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39malias(name)\n",
      "File \u001b[0;32m~/miniforge3/envs/reco/lib/python3.11/site-packages/polars/utils/_parse_expr_input.py:125\u001b[0m, in \u001b[0;36mparse_as_expression\u001b[0;34m(input, str_as_lit, list_as_lit, structify, dtype)\u001b[0m\n\u001b[1;32m    123\u001b[0m     structify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     expr \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     structify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m structify:\n",
      "File \u001b[0;32m~/miniforge3/envs/reco/lib/python3.11/site-packages/polars/functions/lit.py:148\u001b[0m, in \u001b[0;36mlit\u001b[0;34m(value, dtype, allow_object)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     item \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_expr(\u001b[43mplr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: invalid literal value: '<function PPRecDataLoader.transform.<locals>.<lambda> at 0x2d213bec0>'"
     ]
    }
   ],
   "source": [
    "test_PPRecDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
